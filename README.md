# üáßüá∑ Deep Learning para Vis√£o Computacional | üá∫üá∏ Deep Learning for Computer Vision

<div align="center">

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![OpenCV](https://img.shields.io/badge/OpenCV-27338e?style=for-the-badge&logo=opencv&logoColor=white)
![CUDA](https://img.shields.io/badge/CUDA-76B900?style=for-the-badge&logo=nvidia&logoColor=white)

**Plataforma completa de Deep Learning para Vis√£o Computacional com modelos state-of-the-art**

[üî¨ Modelos](#-modelos-implementados) ‚Ä¢ [üìä Datasets](#-datasets) ‚Ä¢ [‚ö° Quick Start](#-quick-start) ‚Ä¢ [üéØ Aplica√ß√µes](#-aplica√ß√µes-pr√°ticas)

</div>

---

## üáßüá∑ Portugu√™s

### üî¨ Vis√£o Geral

Plataforma abrangente de **Deep Learning para Vis√£o Computacional** desenvolvida em Python, implementando arquiteturas state-of-the-art:

- üß† **Redes Neurais Convolucionais**: CNN, ResNet, EfficientNet, Vision Transformer
- üéØ **Detec√ß√£o de Objetos**: YOLO, R-CNN, SSD, RetinaNet
- üñºÔ∏è **Segmenta√ß√£o**: U-Net, Mask R-CNN, DeepLab, Semantic Segmentation
- üëÅÔ∏è **Reconhecimento Facial**: FaceNet, ArcFace, DeepFace
- üé® **Gera√ß√£o de Imagens**: GAN, VAE, Diffusion Models
- üì± **Deploy Mobile**: TensorFlow Lite, ONNX, Edge Computing

### üéØ Objetivos da Plataforma

- **Implementar** arquiteturas modernas de deep learning
- **Facilitar** desenvolvimento de aplica√ß√µes de CV
- **Otimizar** modelos para produ√ß√£o e edge devices
- **Demonstrar** t√©cnicas avan√ßadas de computer vision
- **Acelerar** prototipagem e deployment

### üõ†Ô∏è Stack Tecnol√≥gico

#### Deep Learning Frameworks
- **TensorFlow/Keras**: Framework principal para desenvolvimento
- **PyTorch**: Framework alternativo para pesquisa
- **JAX**: Computa√ß√£o de alto performance
- **Hugging Face Transformers**: Vision Transformers pr√©-treinados

#### Computer Vision
- **OpenCV**: Processamento de imagens cl√°ssico
- **Pillow (PIL)**: Manipula√ß√£o de imagens
- **scikit-image**: Algoritmos de processamento
- **ImageIO**: Leitura/escrita de formatos diversos

#### Visualiza√ß√£o e An√°lise
- **Matplotlib**: Visualiza√ß√£o de resultados
- **Seaborn**: Gr√°ficos estat√≠sticos
- **Plotly**: Visualiza√ß√µes interativas
- **TensorBoard**: Monitoramento de treinamento

#### Deployment e Otimiza√ß√£o
- **TensorFlow Lite**: Modelos para mobile/edge
- **ONNX**: Interoperabilidade entre frameworks
- **TensorRT**: Otimiza√ß√£o para GPUs NVIDIA
- **OpenVINO**: Otimiza√ß√£o para Intel

#### Dados e Augmenta√ß√£o
- **Albumentations**: Augmenta√ß√£o avan√ßada de imagens
- **imgaug**: Augmenta√ß√£o de dados
- **COCO API**: Manipula√ß√£o de datasets COCO
- **Roboflow**: Gerenciamento de datasets

### üìã Estrutura da Plataforma

```
python-deep-learning-cv/
‚îú‚îÄ‚îÄ üìÅ src/                        # C√≥digo fonte principal
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ models/                 # Arquiteturas de modelos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ classification/     # Modelos de classifica√ß√£o
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ resnet.py       # ResNet implementation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ efficientnet.py # EfficientNet implementation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ vision_transformer.py # ViT implementation
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ custom_cnn.py   # CNN customizada
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ detection/          # Modelos de detec√ß√£o
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ yolo_v5.py      # YOLO v5 implementation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ faster_rcnn.py  # Faster R-CNN
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ ssd.py          # Single Shot Detector
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ retinanet.py    # RetinaNet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ segmentation/       # Modelos de segmenta√ß√£o
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ unet.py         # U-Net implementation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ mask_rcnn.py    # Mask R-CNN
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ deeplab.py      # DeepLab v3+
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ fcn.py          # Fully Convolutional Network
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ face_recognition/   # Reconhecimento facial
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ facenet.py      # FaceNet implementation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ arcface.py      # ArcFace implementation
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ deepface.py     # DeepFace wrapper
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ generative/         # Modelos generativos
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ gan.py          # Generative Adversarial Network
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ vae.py          # Variational Autoencoder
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ diffusion.py    # Diffusion Models
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ data/                   # M√≥dulos de dados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ dataset_loader.py   # Carregamento de datasets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ augmentation.py     # Augmenta√ß√£o de dados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ preprocessing.py    # Pr√©-processamento
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ data_utils.py       # Utilit√°rios de dados
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ training/               # M√≥dulos de treinamento
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ trainer.py          # Classe principal de treinamento
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ callbacks.py        # Callbacks customizados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ losses.py           # Fun√ß√µes de perda
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ metrics.py          # M√©tricas de avalia√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ inference/              # M√≥dulos de infer√™ncia
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ predictor.py        # Predi√ß√µes em tempo real
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ batch_inference.py  # Infer√™ncia em lote
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ video_inference.py  # Infer√™ncia em v√≠deo
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ deployment/             # M√≥dulos de deployment
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ model_converter.py  # Convers√£o de modelos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ tflite_converter.py # Convers√£o para TF Lite
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ onnx_converter.py   # Convers√£o para ONNX
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ api_server.py       # Servidor de API
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ utils/                  # Utilit√°rios
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ visualization.py    # Visualiza√ß√£o de resultados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ config.py           # Configura√ß√µes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ logger.py           # Sistema de logs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ helpers.py          # Fun√ß√µes auxiliares
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ evaluation/             # Avalia√ß√£o de modelos
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ evaluator.py        # Avalia√ß√£o completa
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ benchmark.py        # Benchmarking
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ analysis.py         # An√°lise de resultados
‚îú‚îÄ‚îÄ üìÅ datasets/                   # Datasets organizados
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ classification/         # Datasets de classifica√ß√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ cifar10/           # CIFAR-10 dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ imagenet/          # ImageNet subset
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ custom/            # Datasets customizados
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ detection/             # Datasets de detec√ß√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ coco/              # COCO dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ pascal_voc/        # Pascal VOC
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ open_images/       # Open Images
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ segmentation/          # Datasets de segmenta√ß√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ cityscapes/        # Cityscapes dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ ade20k/            # ADE20K dataset
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ medical/           # Datasets m√©dicos
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ faces/                 # Datasets de faces
‚îÇ       ‚îú‚îÄ‚îÄ üìÅ lfw/               # Labeled Faces in the Wild
‚îÇ       ‚îú‚îÄ‚îÄ üìÅ celeba/            # CelebA dataset
‚îÇ       ‚îî‚îÄ‚îÄ üìÅ vggface/           # VGGFace dataset
‚îú‚îÄ‚îÄ üìÅ notebooks/                 # Jupyter notebooks
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 01_data_exploration.ipynb # Explora√ß√£o de dados
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 02_model_training.ipynb # Treinamento de modelos
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 03_transfer_learning.ipynb # Transfer learning
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 04_object_detection.ipynb # Detec√ß√£o de objetos
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 05_image_segmentation.ipynb # Segmenta√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 06_face_recognition.ipynb # Reconhecimento facial
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ 07_generative_models.ipynb # Modelos generativos
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ 08_model_deployment.ipynb # Deployment
‚îú‚îÄ‚îÄ üìÅ experiments/               # Experimentos e resultados
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ classification/        # Experimentos classifica√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ detection/            # Experimentos detec√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ segmentation/         # Experimentos segmenta√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ benchmarks/           # Benchmarks de performance
‚îú‚îÄ‚îÄ üìÅ models/                    # Modelos treinados
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ pretrained/           # Modelos pr√©-treinados
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ checkpoints/          # Checkpoints de treinamento
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ production/           # Modelos em produ√ß√£o
‚îú‚îÄ‚îÄ üìÅ apps/                      # Aplica√ß√µes demo
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ streamlit_app/        # App Streamlit
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ flask_api/            # API Flask
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ mobile_app/           # App mobile (TF Lite)
‚îú‚îÄ‚îÄ üìÅ docker/                    # Containers Docker
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile.gpu        # Container com GPU
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ Dockerfile.cpu        # Container CPU only
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ docker-compose.yml    # Orquestra√ß√£o
‚îú‚îÄ‚îÄ üìÅ tests/                     # Testes automatizados
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_models.py        # Testes de modelos
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_data.py          # Testes de dados
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ test_inference.py     # Testes de infer√™ncia
‚îú‚îÄ‚îÄ üìÅ configs/                   # Arquivos de configura√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ training_config.yaml  # Configura√ß√£o treinamento
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ model_config.yaml     # Configura√ß√£o modelos
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ deployment_config.yaml # Configura√ß√£o deployment
‚îú‚îÄ‚îÄ üìÑ requirements.txt          # Depend√™ncias Python
‚îú‚îÄ‚îÄ üìÑ requirements-gpu.txt      # Depend√™ncias com GPU
‚îú‚îÄ‚îÄ üìÑ setup.py                  # Setup do pacote
‚îú‚îÄ‚îÄ üìÑ README.md                 # Este arquivo
‚îú‚îÄ‚îÄ üìÑ LICENSE                   # Licen√ßa MIT
‚îî‚îÄ‚îÄ üìÑ .gitignore               # Arquivos ignorados
```

### üî¨ Modelos Implementados

#### 1. üñºÔ∏è Classifica√ß√£o de Imagens

**ResNet com Transfer Learning**
```python
import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model

class ResNetClassifier:
    def __init__(self, num_classes, input_shape=(224, 224, 3), weights='imagenet'):
        self.num_classes = num_classes
        self.input_shape = input_shape
        self.weights = weights
        self.model = self._build_model()
    
    def _build_model(self):
        # Base model pr√©-treinada
        base_model = ResNet50(
            weights=self.weights,
            include_top=False,
            input_shape=self.input_shape
        )
        
        # Congelar camadas base para transfer learning
        base_model.trainable = False
        
        # Adicionar camadas customizadas
        x = base_model.output
        x = GlobalAveragePooling2D()(x)
        x = Dense(512, activation='relu')(x)
        x = Dropout(0.5)(x)
        predictions = Dense(self.num_classes, activation='softmax')(x)
        
        model = Model(inputs=base_model.input, outputs=predictions)
        return model
    
    def compile_model(self, learning_rate=0.001):
        self.model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
            loss='categorical_crossentropy',
            metrics=['accuracy', 'top_5_accuracy']
        )
    
    def fine_tune(self, unfreeze_layers=50):
        """Fine-tuning: descongelar √∫ltimas camadas"""
        self.model.layers[0].trainable = True
        
        # Congelar todas exceto as √∫ltimas N camadas
        for layer in self.model.layers[0].layers[:-unfreeze_layers]:
            layer.trainable = False
        
        # Recompilar com learning rate menor
        self.model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
            loss='categorical_crossentropy',
            metrics=['accuracy', 'top_5_accuracy']
        )
```

**Vision Transformer (ViT)**
```python
import tensorflow as tf
from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout
import numpy as np

class PatchEmbedding(Layer):
    def __init__(self, patch_size, embed_dim):
        super().__init__()
        self.patch_size = patch_size
        self.embed_dim = embed_dim
        self.projection = Dense(embed_dim)
        
    def call(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding="VALID"
        )
        
        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        return self.projection(patches)

class MultiHeadAttention(Layer):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        self.qkv = Dense(embed_dim * 3)
        self.projection = Dense(embed_dim)
        
    def call(self, x):
        batch_size, seq_len, _ = x.shape
        
        qkv = self.qkv(x)
        qkv = tf.reshape(qkv, (batch_size, seq_len, 3, self.num_heads, self.head_dim))
        qkv = tf.transpose(qkv, (2, 0, 3, 1, 4))
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        # Scaled dot-product attention
        attention_scores = tf.matmul(q, k, transpose_b=True) / tf.sqrt(tf.cast(self.head_dim, tf.float32))
        attention_weights = tf.nn.softmax(attention_scores, axis=-1)
        attention_output = tf.matmul(attention_weights, v)
        
        # Concatenate heads
        attention_output = tf.transpose(attention_output, (0, 2, 1, 3))
        attention_output = tf.reshape(attention_output, (batch_size, seq_len, self.embed_dim))
        
        return self.projection(attention_output)

class VisionTransformer:
    def __init__(self, image_size, patch_size, num_classes, embed_dim, num_heads, num_layers):
        self.image_size = image_size
        self.patch_size = patch_size
        self.num_classes = num_classes
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.model = self._build_model()
    
    def _build_model(self):
        inputs = tf.keras.Input(shape=(self.image_size, self.image_size, 3))
        
        # Patch embedding
        patches = PatchEmbedding(self.patch_size, self.embed_dim)(inputs)
        
        # Add positional encoding
        num_patches = (self.image_size // self.patch_size) ** 2
        positions = tf.range(start=0, limit=num_patches, delta=1)
        position_embedding = tf.keras.layers.Embedding(
            input_dim=num_patches, output_dim=self.embed_dim
        )(positions)
        
        # Add class token
        class_token = tf.Variable(tf.random.normal([1, 1, self.embed_dim]))
        class_token = tf.tile(class_token, [tf.shape(patches)[0], 1, 1])
        
        encoded_patches = patches + position_embedding
        encoded_patches = tf.concat([class_token, encoded_patches], axis=1)
        
        # Transformer blocks
        for _ in range(self.num_layers):
            # Multi-head attention
            x1 = LayerNormalization(epsilon=1e-6)(encoded_patches)
            attention_output = MultiHeadAttention(self.embed_dim, self.num_heads)(x1)
            x2 = tf.keras.layers.Add()([attention_output, encoded_patches])
            
            # MLP
            x3 = LayerNormalization(epsilon=1e-6)(x2)
            x3 = Dense(self.embed_dim * 4, activation="gelu")(x3)
            x3 = Dropout(0.1)(x3)
            x3 = Dense(self.embed_dim)(x3)
            x3 = Dropout(0.1)(x3)
            encoded_patches = tf.keras.layers.Add()([x3, x2])
        
        # Classification head
        representation = LayerNormalization(epsilon=1e-6)(encoded_patches)
        representation = representation[:, 0]  # Class token
        features = Dense(self.embed_dim, activation="tanh")(representation)
        features = Dropout(0.5)(features)
        logits = Dense(self.num_classes)(features)
        
        model = tf.keras.Model(inputs=inputs, outputs=logits)
        return model
```

#### 2. üéØ Detec√ß√£o de Objetos

**YOLO v5 Implementation**
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)
        self.bn = nn.BatchNorm2d(out_channels)
        self.activation = nn.SiLU()
    
    def forward(self, x):
        return self.activation(self.bn(self.conv(x)))

class CSPBottleneck(nn.Module):
    def __init__(self, in_channels, out_channels, num_blocks=1, shortcut=True):
        super().__init__()
        hidden_channels = out_channels // 2
        
        self.conv1 = ConvBlock(in_channels, hidden_channels, 1)
        self.conv2 = ConvBlock(in_channels, hidden_channels, 1)
        self.conv3 = ConvBlock(2 * hidden_channels, out_channels, 1)
        
        self.bottlenecks = nn.Sequential(*[
            Bottleneck(hidden_channels, hidden_channels, shortcut) 
            for _ in range(num_blocks)
        ])
    
    def forward(self, x):
        y1 = self.conv1(x)
        y2 = self.bottlenecks(self.conv2(x))
        return self.conv3(torch.cat([y1, y2], dim=1))

class YOLOv5:
    def __init__(self, num_classes=80, anchors=None):
        self.num_classes = num_classes
        self.anchors = anchors or [
            [[10, 13], [16, 30], [33, 23]],
            [[30, 61], [62, 45], [59, 119]],
            [[116, 90], [156, 198], [373, 326]]
        ]
        self.model = self._build_model()
    
    def _build_model(self):
        # Backbone (CSPDarknet)
        backbone = self._build_backbone()
        
        # Neck (PANet)
        neck = self._build_neck()
        
        # Head (Detection layers)
        head = self._build_head()
        
        return nn.Sequential(backbone, neck, head)
    
    def _build_backbone(self):
        layers = []
        
        # Stem
        layers.append(ConvBlock(3, 32, 6, 2, 2))
        layers.append(ConvBlock(32, 64, 3, 2, 1))
        
        # Stage 1
        layers.append(CSPBottleneck(64, 64, 1))
        layers.append(ConvBlock(64, 128, 3, 2, 1))
        
        # Stage 2
        layers.append(CSPBottleneck(128, 128, 3))
        layers.append(ConvBlock(128, 256, 3, 2, 1))
        
        # Stage 3
        layers.append(CSPBottleneck(256, 256, 3))
        layers.append(ConvBlock(256, 512, 3, 2, 1))
        
        # Stage 4
        layers.append(CSPBottleneck(512, 512, 1))
        layers.append(ConvBlock(512, 1024, 3, 2, 1))
        
        # Stage 5
        layers.append(CSPBottleneck(1024, 1024, 1))
        
        return nn.Sequential(*layers)
    
    def detect_objects(self, image, confidence_threshold=0.5, nms_threshold=0.4):
        """Detectar objetos em uma imagem"""
        with torch.no_grad():
            predictions = self.model(image)
            detections = self._post_process(predictions, confidence_threshold, nms_threshold)
        return detections
    
    def _post_process(self, predictions, conf_thresh, nms_thresh):
        """P√≥s-processamento das predi√ß√µes"""
        # Non-Maximum Suppression
        detections = []
        
        for pred in predictions:
            # Filtrar por confian√ßa
            conf_mask = pred[..., 4] > conf_thresh
            pred = pred[conf_mask]
            
            if len(pred) == 0:
                continue
            
            # Converter coordenadas
            boxes = self._xywh_to_xyxy(pred[..., :4])
            scores = pred[..., 4:5] * pred[..., 5:]
            
            # NMS
            keep = self._nms(boxes, scores.max(1)[0], nms_thresh)
            detections.append(pred[keep])
        
        return detections
```

#### 3. üñºÔ∏è Segmenta√ß√£o Sem√¢ntica

**U-Net para Segmenta√ß√£o M√©dica**
```python
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, concatenate

class UNet:
    def __init__(self, input_shape, num_classes):
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.model = self._build_model()
    
    def _conv_block(self, inputs, filters, kernel_size=3):
        """Bloco convolucional padr√£o"""
        x = Conv2D(filters, kernel_size, activation='relu', padding='same')(inputs)
        x = Conv2D(filters, kernel_size, activation='relu', padding='same')(x)
        return x
    
    def _encoder_block(self, inputs, filters):
        """Bloco do encoder"""
        conv = self._conv_block(inputs, filters)
        pool = MaxPooling2D(pool_size=(2, 2))(conv)
        return conv, pool
    
    def _decoder_block(self, inputs, skip_connection, filters):
        """Bloco do decoder"""
        up = UpSampling2D(size=(2, 2))(inputs)
        up = Conv2D(filters, 2, activation='relu', padding='same')(up)
        
        # Skip connection
        merge = concatenate([skip_connection, up], axis=3)
        conv = self._conv_block(merge, filters)
        return conv
    
    def _build_model(self):
        inputs = tf.keras.Input(self.input_shape)
        
        # Encoder (Contracting path)
        conv1, pool1 = self._encoder_block(inputs, 64)
        conv2, pool2 = self._encoder_block(pool1, 128)
        conv3, pool3 = self._encoder_block(pool2, 256)
        conv4, pool4 = self._encoder_block(pool3, 512)
        
        # Bottleneck
        conv5 = self._conv_block(pool4, 1024)
        
        # Decoder (Expanding path)
        up6 = self._decoder_block(conv5, conv4, 512)
        up7 = self._decoder_block(up6, conv3, 256)
        up8 = self._decoder_block(up7, conv2, 128)
        up9 = self._decoder_block(up8, conv1, 64)
        
        # Output layer
        outputs = Conv2D(self.num_classes, 1, activation='softmax')(up9)
        
        model = tf.keras.Model(inputs=inputs, outputs=outputs)
        return model
    
    def compile_model(self):
        self.model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy', self._dice_coefficient, self._iou_score]
        )
    
    def _dice_coefficient(self, y_true, y_pred, smooth=1):
        """Coeficiente Dice para segmenta√ß√£o"""
        y_true_f = tf.keras.backend.flatten(y_true)
        y_pred_f = tf.keras.backend.flatten(y_pred)
        intersection = tf.keras.backend.sum(y_true_f * y_pred_f)
        return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)
    
    def _iou_score(self, y_true, y_pred, smooth=1):
        """Intersection over Union"""
        y_true_f = tf.keras.backend.flatten(y_true)
        y_pred_f = tf.keras.backend.flatten(y_pred)
        intersection = tf.keras.backend.sum(y_true_f * y_pred_f)
        union = tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) - intersection
        return (intersection + smooth) / (union + smooth)
```

#### 4. üëÅÔ∏è Reconhecimento Facial

**FaceNet Implementation**
```python
import tensorflow as tf
from tensorflow.keras.layers import Dense, Lambda, Input
from tensorflow.keras.models import Model
import numpy as np

class FaceNet:
    def __init__(self, input_shape=(160, 160, 3), embedding_size=128):
        self.input_shape = input_shape
        self.embedding_size = embedding_size
        self.model = self._build_model()
    
    def _build_model(self):
        # Base model (pode ser ResNet, Inception, etc.)
        base_model = tf.keras.applications.InceptionResNetV2(
            input_shape=self.input_shape,
            include_top=False,
            weights='imagenet'
        )
        
        # Adicionar camadas de embedding
        x = base_model.output
        x = tf.keras.layers.GlobalAveragePooling2D()(x)
        x = Dense(self.embedding_size)(x)
        
        # L2 normalization
        embeddings = Lambda(lambda x: tf.nn.l2_normalize(x, axis=1))(x)
        
        model = Model(inputs=base_model.input, outputs=embeddings)
        return model
    
    def triplet_loss(self, alpha=0.2):
        """Triplet loss para treinamento"""
        def loss(y_true, y_pred):
            anchor, positive, negative = y_pred[:, :self.embedding_size], \
                                       y_pred[:, self.embedding_size:2*self.embedding_size], \
                                       y_pred[:, 2*self.embedding_size:]
            
            # Dist√¢ncias
            pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)
            neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)
            
            # Triplet loss
            basic_loss = pos_dist - neg_dist + alpha
            loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0))
            
            return loss
        return loss
    
    def get_embedding(self, face_image):
        """Obter embedding de uma face"""
        if len(face_image.shape) == 3:
            face_image = np.expand_dims(face_image, axis=0)
        
        embedding = self.model.predict(face_image)
        return embedding[0]
    
    def compare_faces(self, face1, face2, threshold=0.6):
        """Comparar duas faces"""
        emb1 = self.get_embedding(face1)
        emb2 = self.get_embedding(face2)
        
        distance = np.linalg.norm(emb1 - emb2)
        is_same_person = distance < threshold
        
        return {
            'distance': distance,
            'is_same_person': is_same_person,
            'confidence': 1 - (distance / 2)  # Normalizar para 0-1
        }
    
    def face_recognition_pipeline(self, image, known_faces_db):
        """Pipeline completo de reconhecimento"""
        # 1. Detectar faces na imagem
        faces = self._detect_faces(image)
        
        results = []
        for face in faces:
            # 2. Obter embedding da face
            embedding = self.get_embedding(face['image'])
            
            # 3. Comparar com banco de faces conhecidas
            best_match = None
            min_distance = float('inf')
            
            for person_id, known_embedding in known_faces_db.items():
                distance = np.linalg.norm(embedding - known_embedding)
                if distance < min_distance:
                    min_distance = distance
                    best_match = person_id
            
            # 4. Determinar se √© uma pessoa conhecida
            if min_distance < 0.6:  # threshold
                results.append({
                    'bbox': face['bbox'],
                    'person_id': best_match,
                    'confidence': 1 - (min_distance / 2),
                    'distance': min_distance
                })
            else:
                results.append({
                    'bbox': face['bbox'],
                    'person_id': 'unknown',
                    'confidence': 0,
                    'distance': min_distance
                })
        
        return results
```

### üéØ Aplica√ß√µes Pr√°ticas

#### 1. üè• Diagn√≥stico M√©dico por Imagem

**Classifica√ß√£o de Raios-X**
```python
class MedicalImageClassifier:
    def __init__(self):
        self.model = self._load_pretrained_model()
        self.classes = ['Normal', 'Pneumonia', 'COVID-19', 'Tuberculosis']
    
    def diagnose_xray(self, xray_image):
        # Pr√©-processamento
        processed_image = self._preprocess_medical_image(xray_image)
        
        # Predi√ß√£o
        predictions = self.model.predict(processed_image)
        
        # Interpreta√ß√£o
        diagnosis = {
            'primary_diagnosis': self.classes[np.argmax(predictions)],
            'confidence': float(np.max(predictions)),
            'all_probabilities': {
                class_name: float(prob) 
                for class_name, prob in zip(self.classes, predictions[0])
            }
        }
        
        # Mapa de aten√ß√£o (Grad-CAM)
        attention_map = self._generate_gradcam(processed_image)
        
        return {
            'diagnosis': diagnosis,
            'attention_map': attention_map,
            'recommendations': self._get_recommendations(diagnosis)
        }
    
    def _generate_gradcam(self, image):
        """Gerar mapa de aten√ß√£o Grad-CAM"""
        # Implementa√ß√£o Grad-CAM para interpretabilidade
        pass
```

#### 2. üöó Vis√£o Computacional Automotiva

**Detec√ß√£o para Ve√≠culos Aut√¥nomos**
```python
class AutomotiveVision:
    def __init__(self):
        self.object_detector = self._load_object_detector()
        self.lane_detector = self._load_lane_detector()
        self.traffic_sign_classifier = self._load_traffic_sign_classifier()
    
    def process_driving_scene(self, image):
        results = {}
        
        # Detec√ß√£o de objetos
        objects = self.object_detector.detect(image)
        results['objects'] = self._filter_automotive_objects(objects)
        
        # Detec√ß√£o de faixas
        lanes = self.lane_detector.detect_lanes(image)
        results['lanes'] = lanes
        
        # Classifica√ß√£o de sinais de tr√¢nsito
        traffic_signs = self.traffic_sign_classifier.detect_and_classify(image)
        results['traffic_signs'] = traffic_signs
        
        # An√°lise de risco
        risk_assessment = self._assess_driving_risk(results)
        results['risk_assessment'] = risk_assessment
        
        return results
    
    def _assess_driving_risk(self, detection_results):
        """Avaliar risco baseado nas detec√ß√µes"""
        risk_factors = []
        
        # Verificar proximidade de pedestres
        for obj in detection_results['objects']:
            if obj['class'] == 'person' and obj['distance'] < 10:
                risk_factors.append('pedestrian_close')
        
        # Verificar ve√≠culos pr√≥ximos
        for obj in detection_results['objects']:
            if obj['class'] in ['car', 'truck'] and obj['distance'] < 5:
                risk_factors.append('vehicle_close')
        
        # Verificar sinais de tr√¢nsito
        for sign in detection_results['traffic_signs']:
            if sign['class'] == 'stop_sign':
                risk_factors.append('stop_sign_detected')
        
        return {
            'risk_level': len(risk_factors),
            'risk_factors': risk_factors,
            'recommended_action': self._get_recommended_action(risk_factors)
        }
```

#### 3. üè≠ Controle de Qualidade Industrial

**Inspe√ß√£o Automatizada**
```python
class QualityInspection:
    def __init__(self):
        self.defect_detector = self._load_defect_detection_model()
        self.measurement_model = self._load_measurement_model()
    
    def inspect_product(self, product_image):
        inspection_results = {}
        
        # Detec√ß√£o de defeitos
        defects = self.defect_detector.detect_defects(product_image)
        inspection_results['defects'] = defects
        
        # Medi√ß√µes dimensionais
        measurements = self.measurement_model.measure_dimensions(product_image)
        inspection_results['measurements'] = measurements
        
        # Classifica√ß√£o de qualidade
        quality_score = self._calculate_quality_score(defects, measurements)
        inspection_results['quality_score'] = quality_score
        
        # Decis√£o de aprova√ß√£o/rejei√ß√£o
        decision = self._make_quality_decision(quality_score, defects)
        inspection_results['decision'] = decision
        
        return inspection_results
    
    def _calculate_quality_score(self, defects, measurements):
        """Calcular score de qualidade baseado em defeitos e medi√ß√µes"""
        base_score = 100
        
        # Penalizar por defeitos
        for defect in defects:
            severity = defect['severity']
            base_score -= severity * 10
        
        # Penalizar por medi√ß√µes fora de especifica√ß√£o
        for measurement in measurements:
            if not measurement['within_tolerance']:
                deviation = measurement['deviation_percentage']
                base_score -= deviation * 5
        
        return max(0, base_score)
```

### üöÄ Deployment e Otimiza√ß√£o

#### TensorFlow Lite para Mobile
```python
class MobileDeployment:
    def __init__(self, model_path):
        self.model_path = model_path
    
    def convert_to_tflite(self, quantization=True):
        """Converter modelo para TensorFlow Lite"""
        converter = tf.lite.TFLiteConverter.from_saved_model(self.model_path)
        
        if quantization:
            # Quantiza√ß√£o para reduzir tamanho
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            
            # Quantiza√ß√£o INT8 (opcional)
            converter.target_spec.supported_types = [tf.int8]
            converter.inference_input_type = tf.uint8
            converter.inference_output_type = tf.uint8
        
        tflite_model = converter.convert()
        
        # Salvar modelo
        tflite_path = self.model_path.replace('.pb', '.tflite')
        with open(tflite_path, 'wb') as f:
            f.write(tflite_model)
        
        return tflite_path
    
    def benchmark_model(self, tflite_path, test_images):
        """Benchmark de performance do modelo"""
        interpreter = tf.lite.Interpreter(model_path=tflite_path)
        interpreter.allocate_tensors()
        
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        
        inference_times = []
        
        for image in test_images:
            start_time = time.time()
            
            # Preparar input
            interpreter.set_tensor(input_details[0]['index'], image)
            
            # Executar infer√™ncia
            interpreter.invoke()
            
            # Obter output
            output = interpreter.get_tensor(output_details[0]['index'])
            
            inference_time = time.time() - start_time
            inference_times.append(inference_time)
        
        return {
            'avg_inference_time': np.mean(inference_times),
            'min_inference_time': np.min(inference_times),
            'max_inference_time': np.max(inference_times),
            'fps': 1.0 / np.mean(inference_times)
        }
```

### üéØ Compet√™ncias Demonstradas

#### Deep Learning
- ‚úÖ **CNNs**: Redes convolucionais cl√°ssicas e modernas
- ‚úÖ **Transfer Learning**: Aproveitamento de modelos pr√©-treinados
- ‚úÖ **Vision Transformers**: Arquiteturas baseadas em aten√ß√£o
- ‚úÖ **GANs**: Redes advers√°rias generativas

#### Computer Vision
- ‚úÖ **Classifica√ß√£o**: Reconhecimento de imagens
- ‚úÖ **Detec√ß√£o**: Localiza√ß√£o e classifica√ß√£o de objetos
- ‚úÖ **Segmenta√ß√£o**: Segmenta√ß√£o sem√¢ntica e de inst√¢ncia
- ‚úÖ **Reconhecimento Facial**: Identifica√ß√£o e verifica√ß√£o

#### MLOps para CV
- ‚úÖ **Model Optimization**: Quantiza√ß√£o, pruning, distillation
- ‚úÖ **Edge Deployment**: TensorFlow Lite, ONNX
- ‚úÖ **Performance Monitoring**: Lat√™ncia, throughput, accuracy
- ‚úÖ **A/B Testing**: Compara√ß√£o de modelos em produ√ß√£o

### üìä Benchmarks de Performance

#### Modelos de Classifica√ß√£o
| Modelo | ImageNet Top-1 | Par√¢metros | FLOPs | Lat√™ncia (ms) |
|--------|----------------|------------|-------|---------------|
| ResNet-50 | 76.1% | 25.6M | 4.1G | 15.2 |
| EfficientNet-B0 | 77.1% | 5.3M | 0.39G | 8.7 |
| Vision Transformer | 81.8% | 86M | 17.6G | 45.3 |

#### Modelos de Detec√ß√£o
| Modelo | COCO mAP | FPS | Tamanho |
|--------|----------|-----|---------|
| YOLOv5s | 37.4 | 140 | 14MB |
| YOLOv5m | 45.4 | 85 | 42MB |
| YOLOv5l | 49.0 | 55 | 92MB |

---

## üá∫üá∏ English

### üî¨ Overview

Comprehensive **Deep Learning for Computer Vision** platform developed in Python, implementing state-of-the-art architectures:

- üß† **Convolutional Neural Networks**: CNN, ResNet, EfficientNet, Vision Transformer
- üéØ **Object Detection**: YOLO, R-CNN, SSD, RetinaNet
- üñºÔ∏è **Segmentation**: U-Net, Mask R-CNN, DeepLab, Semantic Segmentation
- üëÅÔ∏è **Face Recognition**: FaceNet, ArcFace, DeepFace
- üé® **Image Generation**: GAN, VAE, Diffusion Models
- üì± **Mobile Deploy**: TensorFlow Lite, ONNX, Edge Computing

### üéØ Platform Objectives

- **Implement** modern deep learning architectures
- **Facilitate** CV application development
- **Optimize** models for production and edge devices
- **Demonstrate** advanced computer vision techniques
- **Accelerate** prototyping and deployment

### üî¨ Implemented Models

#### 1. üñºÔ∏è Image Classification
- ResNet with Transfer Learning
- EfficientNet for efficiency
- Vision Transformer (ViT)
- Custom CNN architectures

#### 2. üéØ Object Detection
- YOLO v5 implementation
- Faster R-CNN
- Single Shot Detector (SSD)
- RetinaNet with focal loss

#### 3. üñºÔ∏è Semantic Segmentation
- U-Net for medical imaging
- Mask R-CNN for instance segmentation
- DeepLab v3+ for semantic segmentation
- Fully Convolutional Networks

#### 4. üëÅÔ∏è Face Recognition
- FaceNet implementation
- ArcFace for face verification
- DeepFace wrapper
- Face detection and alignment

### üéØ Skills Demonstrated

#### Deep Learning
- ‚úÖ **CNNs**: Classical and modern convolutional networks
- ‚úÖ **Transfer Learning**: Leveraging pre-trained models
- ‚úÖ **Vision Transformers**: Attention-based architectures
- ‚úÖ **GANs**: Generative adversarial networks

#### Computer Vision
- ‚úÖ **Classification**: Image recognition
- ‚úÖ **Detection**: Object localization and classification
- ‚úÖ **Segmentation**: Semantic and instance segmentation
- ‚úÖ **Face Recognition**: Identification and verification

#### MLOps for CV
- ‚úÖ **Model Optimization**: Quantization, pruning, distillation
- ‚úÖ **Edge Deployment**: TensorFlow Lite, ONNX
- ‚úÖ **Performance Monitoring**: Latency, throughput, accuracy
- ‚úÖ **A/B Testing**: Model comparison in production

---

## üìÑ Licen√ßa | License

MIT License - veja o arquivo [LICENSE](LICENSE) para detalhes | see [LICENSE](LICENSE) file for details

## üìû Contato | Contact

**GitHub**: [@galafis](https://github.com/galafis)  
**LinkedIn**: [Gabriel Demetrios Lafis](https://linkedin.com/in/galafis)  
**Email**: gabriel.lafis@example.com

---

<div align="center">

**Desenvolvido com ‚ù§Ô∏è para Vis√£o Computacional | Developed with ‚ù§Ô∏è for Computer Vision**

[![GitHub](https://img.shields.io/badge/GitHub-galafis-blue?style=flat-square&logo=github)](https://github.com/galafis)
[![Python](https://img.shields.io/badge/Python-3776AB?style=flat-square&logo=python&logoColor=white)](https://www.python.org/)

</div>

