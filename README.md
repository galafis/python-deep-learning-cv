_# ğŸ‡§ğŸ‡· Deep Learning para VisÃ£o Computacional | ğŸ‡ºğŸ‡¸ Deep Learning for Computer Vision

<div align="center">

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![OpenCV](https://img.shields.io/badge/OpenCV-27338e?style=for-the-badge&logo=opencv&logoColor=white)
![CUDA](https://img.shields.io/badge/CUDA-76B900?style=for-the-badge&logo=nvidia&logoColor=white)

**Plataforma completa de Deep Learning para VisÃ£o Computacional com modelos state-of-the-art**

[ğŸ”¬ Modelos](#-modelos-implementados) â€¢ [ğŸ“Š Datasets](#-datasets) â€¢ [âš¡ Quick Start](#-quick-start) â€¢ [ğŸ¯ AplicaÃ§Ãµes](#-aplicaÃ§Ãµes-prÃ¡ticas)

</div>

---

## ğŸ‡§ğŸ‡· PortuguÃªs

### ğŸ”¬ VisÃ£o Geral

Plataforma abrangente de **Deep Learning para VisÃ£o Computacional** desenvolvida em Python, implementando arquiteturas state-of-the-art:

- ğŸ§  **Redes Neurais Convolucionais**: CNN, ResNet, EfficientNet, Vision Transformer
- ğŸ¯ **DetecÃ§Ã£o de Objetos**: YOLO, R-CNN, SSD, RetinaNet
- ğŸ–¼ï¸ **SegmentaÃ§Ã£o**: U-Net, Mask R-CNN, DeepLab, Semantic Segmentation
- ğŸ‘ï¸ **Reconhecimento Facial**: FaceNet, ArcFace, DeepFace
- ğŸ¨ **GeraÃ§Ã£o de Imagens**: GAN, VAE, Diffusion Models
- ğŸ“± **Deploy Mobile**: TensorFlow Lite, ONNX, Edge Computing

### ğŸ¯ Objetivos da Plataforma

- **Implementar** arquiteturas modernas de deep learning
- **Facilitar** desenvolvimento de aplicaÃ§Ãµes de CV
- **Otimizar** modelos para produÃ§Ã£o e edge devices
- **Demonstrar** tÃ©cnicas avanÃ§adas de computer vision
- **Acelerar** prototipagem e deployment

### ğŸ› ï¸ Stack TecnolÃ³gico

#### Deep Learning Frameworks
- **TensorFlow/Keras**: Framework principal para desenvolvimento
- **PyTorch**: Framework alternativo para pesquisa
- **JAX**: ComputaÃ§Ã£o de alto performance
- **Hugging Face Transformers**: Vision Transformers prÃ©-treinados

#### Computer Vision
- **OpenCV**: Processamento de imagens clÃ¡ssico
- **Pillow (PIL)**: ManipulaÃ§Ã£o de imagens
- **scikit-image**: Algoritmos de processamento
- **ImageIO**: Leitura/escrita de formatos diversos

#### VisualizaÃ§Ã£o e AnÃ¡lise
- **Matplotlib**: VisualizaÃ§Ã£o de resultados
- **Seaborn**: GrÃ¡ficos estatÃ­sticos
- **Plotly**: VisualizaÃ§Ãµes interativas
- **TensorBoard**: Monitoramento de treinamento

#### Deployment e OtimizaÃ§Ã£o
- **TensorFlow Lite**: Modelos para mobile/edge
- **ONNX**: Interoperabilidade entre frameworks
- **TensorRT**: OtimizaÃ§Ã£o para GPUs NVIDIA
- **OpenVINO**: OtimizaÃ§Ã£o para Intel

#### Dados e AugmentaÃ§Ã£o
- **Albumentations**: AugmentaÃ§Ã£o avanÃ§ada de imagens
- **imgaug**: AugmentaÃ§Ã£o de dados
- **COCO API**: ManipulaÃ§Ã£o de datasets COCO
- **Roboflow**: Gerenciamento de datasets

### ğŸ“‹ Estrutura da Plataforma

```
python-deep-learning-cv/
â”œâ”€â”€ ğŸ“ src/                        # CÃ³digo fonte principal
â”‚   â”œâ”€â”€ ğŸ“ models/                 # Arquiteturas de modelos
â”‚   â”‚   â”œâ”€â”€ ğŸ“ classification/     # Modelos de classificaÃ§Ã£o
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ resnet.py       # ResNet implementation
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ efficientnet.py # EfficientNet implementation
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ vision_transformer.py # ViT implementation
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ custom_cnn.py   # CNN customizada
â”‚   â”‚   â”œâ”€â”€ ğŸ“ detection/          # Modelos de detecÃ§Ã£o
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ yolo_v5.py      # YOLO v5 implementation
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ faster_rcnn.py  # Faster R-CNN
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ ssd.py          # Single Shot Detector
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ retinanet.py    # RetinaNet
â”‚   â”‚   â”œâ”€â”€ ğŸ“ segmentation/       # Modelos de segmentaÃ§Ã£o
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ unet.py         # U-Net implementation
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ mask_rcnn.py    # Mask R-CNN
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ deeplab.py      # DeepLab v3+
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ fcn.py          # Fully Convolutional Network
â”‚   â”‚   â”œâ”€â”€ ğŸ“ face_recognition/   # Reconhecimento facial
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ facenet.py      # FaceNet implementation
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ arcface.py      # ArcFace implementation
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ deepface.py     # DeepFace wrapper
â”‚   â”‚   â””â”€â”€ ğŸ“ generative/         # Modelos generativos
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ gan.py          # Generative Adversarial Network
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ vae.py          # Variational Autoencoder
â”‚   â”‚       â””â”€â”€ ğŸ“„ diffusion.py    # Diffusion Models
â”‚   â”œâ”€â”€ ğŸ“ data/                   # MÃ³dulos de dados
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dataset_loader.py   # Carregamento de datasets
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ augmentation.py     # AugmentaÃ§Ã£o de dados
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ preprocessing.py    # PrÃ©-processamento
â”‚   â”‚   â””â”€â”€ ğŸ“„ data_utils.py       # UtilitÃ¡rios de dados
â”‚   â”œâ”€â”€ ğŸ“ training/               # MÃ³dulos de treinamento
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ trainer.py          # Classe principal de treinamento
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ callbacks.py        # Callbacks customizados
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ losses.py           # FunÃ§Ãµes de perda
â”‚   â”‚   â””â”€â”€ ğŸ“„ metrics.py          # MÃ©tricas de avaliaÃ§Ã£o
â”‚   â”œâ”€â”€ ğŸ“ inference/              # MÃ³dulos de inferÃªncia
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ predictor.py        # PrediÃ§Ãµes em tempo real
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ batch_inference.py  # InferÃªncia em lote
â”‚   â”‚   â””â”€â”€ ğŸ“„ video_inference.py  # InferÃªncia em vÃ­deo
â”‚   â”œâ”€â”€ ğŸ“ deployment/             # MÃ³dulos de deployment
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ model_converter.py  # ConversÃ£o de modelos
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ tflite_converter.py # ConversÃ£o para TF Lite
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ onnx_converter.py   # ConversÃ£o para ONNX
â”‚   â”‚   â””â”€â”€ ğŸ“„ api_server.py       # Servidor de API
â”‚   â”œâ”€â”€ ğŸ“ utils/                  # UtilitÃ¡rios
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ visualization.py    # VisualizaÃ§Ã£o de resultados
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ config.py           # ConfiguraÃ§Ãµes
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ logger.py           # Sistema de logs
â”‚   â”‚   â””â”€â”€ ğŸ“„ helpers.py          # FunÃ§Ãµes auxiliares
â”‚   â””â”€â”€ ğŸ“ evaluation/             # AvaliaÃ§Ã£o de modelos
â”‚       â”œâ”€â”€ ğŸ“„ evaluator.py        # AvaliaÃ§Ã£o completa
â”‚       â”œâ”€â”€ ğŸ“„ benchmark.py        # Benchmarking
â”‚       â””â”€â”€ ğŸ“„ analysis.py         # AnÃ¡lise de resultados
â”œâ”€â”€ ğŸ“ datasets/                   # Datasets organizados
â”‚   â”œâ”€â”€ ğŸ“ classification/         # Datasets de classificaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ ğŸ“ cifar10/           # CIFAR-10 dataset
â”‚   â”‚   â”œâ”€â”€ ğŸ“ imagenet/          # ImageNet subset
â”‚   â”‚   â””â”€â”€ ğŸ“ custom/            # Datasets customizados
â”‚   â”œâ”€â”€ ğŸ“ detection/             # Datasets de detecÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ ğŸ“ coco/              # COCO dataset
â”‚   â”‚   â”œâ”€â”€ ğŸ“ pascal_voc/        # Pascal VOC
â”‚   â”‚   â””â”€â”€ ğŸ“ open_images/       # Open Images
â”‚   â”œâ”€â”€ ğŸ“ segmentation/          # Datasets de segmentaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ ğŸ“ cityscapes/        # Cityscapes dataset
â”‚   â”‚   â”œâ”€â”€ ğŸ“ ade20k/            # ADE20K dataset
â”‚   â”‚   â””â”€â”€ ğŸ“ medical/           # Datasets mÃ©dicos
â”‚   â””â”€â”€ ğŸ“ faces/                 # Datasets de faces
â”‚       â”œâ”€â”€ ğŸ“ lfw/               # Labeled Faces in the Wild
â”‚       â”œâ”€â”€ ğŸ“ celeba/            # CelebA dataset
â”‚       â””â”€â”€ ğŸ“ vggface/           # VGGFace dataset
â”œâ”€â”€ ğŸ“ notebooks/                 # Jupyter notebooks
â”‚   â”œâ”€â”€ ğŸ“„ 01_data_exploration.ipynb # ExploraÃ§Ã£o de dados
â”‚   â”œâ”€â”€ ğŸ“„ 02_model_training.ipynb # Treinamento de modelos
â”‚   â”œâ”€â”€ ğŸ“„ 03_transfer_learning.ipynb # Transfer learning
â”‚   â”œâ”€â”€ ğŸ“„ 04_object_detection.ipynb # DetecÃ§Ã£o de objetos
â”‚   â”œâ”€â”€ ğŸ“„ 05_image_segmentation.ipynb # SegmentaÃ§Ã£o
â”‚   â”œâ”€â”€ ğŸ“„ 06_face_recognition.ipynb # Reconhecimento facial
â”‚   â”œâ”€â”€ ğŸ“„ 07_generative_models.ipynb # Modelos generativos
â”‚   â””â”€â”€ ğŸ“„ 08_model_deployment.ipynb # Deployment
â”œâ”€â”€ ğŸ“ experiments/               # Experimentos e resultados
â”‚   â”œâ”€â”€ ğŸ“ classification/        # Experimentos classificaÃ§Ã£o
â”‚   â”œâ”€â”€ ğŸ“ detection/            # Experimentos detecÃ§Ã£o
â”‚   â”œâ”€â”€ ğŸ“ segmentation/         # Experimentos segmentaÃ§Ã£o
â”‚   â””â”€â”€ ğŸ“ benchmarks/           # Benchmarks de performance
â”œâ”€â”€ ğŸ“ models/                    # Modelos treinados
â”‚   â”œâ”€â”€ ğŸ“ pretrained/           # Modelos prÃ©-treinados
â”‚   â”œâ”€â”€ ğŸ“ checkpoints/          # Checkpoints de treinamento
â”‚   â””â”€â”€ ğŸ“ production/           # Modelos em produÃ§Ã£o
â”œâ”€â”€ ğŸ“ apps/                      # AplicaÃ§Ãµes demo
â”‚   â”œâ”€â”€ ğŸ“ streamlit_app/        # App Streamlit
â”‚   â”œâ”€â”€ ğŸ“ flask_api/            # API Flask
â”‚   â””â”€â”€ ğŸ“ mobile_app/           # App mobile (TF Lite)
â”œâ”€â”€ ğŸ“ docker/                    # Containers Docker
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile.gpu        # Container com GPU
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile.cpu        # Container CPU only
â”‚   â””â”€â”€ ğŸ“„ docker-compose.yml    # OrquestraÃ§Ã£o
â”œâ”€â”€ ğŸ“ tests/                     # Testes automatizados
â”‚   â”œâ”€â”€ ğŸ“„ test_models.py        # Testes de modelos
â”‚   â”œâ”€â”€ ğŸ“„ test_data.py          # Testes de dados
â”‚   â””â”€â”€ ğŸ“„ test_inference.py     # Testes de inferÃªncia
â”œâ”€â”€ ğŸ“ configs/                   # Arquivos de configuraÃ§Ã£o
â”‚   â”œâ”€â”€ ğŸ“„ training_config.yaml  # ConfiguraÃ§Ã£o treinamento
â”‚   â”œâ”€â”€ ğŸ“„ model_config.yaml     # ConfiguraÃ§Ã£o modelos
â”‚   â””â”€â”€ ğŸ“„ deployment_config.yaml # ConfiguraÃ§Ã£o deployment
â”œâ”€â”€ ğŸ“„ requirements.txt          # DependÃªncias Python
â”œâ”€â”€ ğŸ“„ requirements-gpu.txt      # DependÃªncias com GPU
â”œâ”€â”€ ğŸ“„ setup.py                  # Setup do pacote
â”œâ”€â”€ ğŸ“„ README.md                 # Este arquivo
â”œâ”€â”€ ğŸ“„ LICENSE                   # LicenÃ§a MIT
â””â”€â”€ ğŸ“„ .gitignore               # Arquivos ignorados
```

### ğŸ”¬ Modelos Implementados

#### 1. ğŸ–¼ï¸ ClassificaÃ§Ã£o de Imagens

**ResNet com Transfer Learning**
```python
import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model

class ResNetClassifier:
    def __init__(self, num_classes, input_shape=(224, 224, 3), weights='imagenet'):
        self.num_classes = num_classes
        self.input_shape = input_shape
        self.weights = weights
        self.model = self._build_model()
    
    def _build_model(self):
        # Base model prÃ©-treinada
        base_model = ResNet50(
            weights=self.weights,
            include_top=False,
            input_shape=self.input_shape
        )
        
        # Congelar camadas base para transfer learning
        base_model.trainable = False
        
        # Adicionar camadas customizadas
        x = base_model.output
        x = GlobalAveragePooling2D()(x)
        x = Dense(512, activation='relu')(x)
        x = Dropout(0.5)(x)
        predictions = Dense(self.num_classes, activation='softmax')(x)
        
        model = Model(inputs=base_model.input, outputs=predictions)
        return model
    
    def compile_model(self, learning_rate=0.001):
        self.model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
            loss='categorical_crossentropy',
            metrics=['accuracy', 'top_5_accuracy']
        )
    
    def fine_tune(self, unfreeze_layers=50):
        "'''Fine-tuning: descongelar Ãºltimas camadas"'''
        self.model.layers[0].trainable = True
        
        # Congelar todas exceto as Ãºltimas N camadas
        for layer in self.model.layers[0].layers[:-unfreeze_layers]:
            layer.trainable = False
        
        # Recompilar com learning rate menor
        self.model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
            loss='categorical_crossentropy',
            metrics=['accuracy', 'top_5_accuracy']
        )
```

**Vision Transformer (ViT)**
```python
import tensorflow as tf
from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout
import numpy as np

class PatchEmbedding(Layer):
    def __init__(self, patch_size, embed_dim):
        super().__init__()
        self.patch_size = patch_size
        self.embed_dim = embed_dim
        self.projection = Dense(embed_dim)
        
    def call(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding="VALID"
        )
        
        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        return self.projection(patches)

class MultiHeadAttention(Layer):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        
        self.qkv = Dense(embed_dim * 3)
        self.projection = Dense(embed_dim)
        
    def call(self, x):
        batch_size, seq_len, _ = x.shape
        
        qkv = self.qkv(x)
        qkv = tf.reshape(qkv, (batch_size, seq_len, 3, self.num_heads, self.head_dim))
        qkv = tf.transpose(qkv, (2, 0, 3, 1, 4))
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        # Scaled dot-product attention
        attention_scores = tf.matmul(q, k, transpose_b=True) / tf.sqrt(tf.cast(self.head_dim, tf.float32))
        attention_weights = tf.nn.softmax(attention_scores, axis=-1)
        attention_output = tf.matmul(attention_weights, v)
        
        # Concatenate heads
        attention_output = tf.transpose(attention_output, (0, 2, 1, 3))
        attention_output = tf.reshape(attention_output, (batch_size, seq_len, self.embed_dim))
        
        return self.projection(attention_output)

class VisionTransformer:
    def __init__(self, image_size, patch_size, num_classes, embed_dim, num_heads, num_layers):
        self.image_size = image_size
        self.patch_size = patch_size
        self.num_classes = num_classes
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.model = self._build_model()
    
    def _build_model(self):
        inputs = tf.keras.Input(shape=(self.image_size, self.image_size, 3))
        
        # Patch embedding
        patches = PatchEmbedding(self.patch_size, self.embed_dim)(inputs)
        
        # Add positional encoding
        num_patches = (self.image_size // self.patch_size) ** 2
        positions = tf.range(start=0, limit=num_patches, delta=1)
        position_embedding = tf.keras.layers.Embedding(
            input_dim=num_patches, output_dim=self.embed_dim
        )(positions)
        
        # Add class token
        class_token = tf.Variable(tf.random.normal([1, 1, self.embed_dim]))
        class_token = tf.tile(class_token, [tf.shape(patches)[0], 1, 1])
        
        encoded_patches = patches + position_embedding
        encoded_patches = tf.concat([class_token, encoded_patches], axis=1)
        
        # Transformer blocks
        for _ in range(self.num_layers):
            # Multi-head attention
            x1 = LayerNormalization(epsilon=1e-6)(encoded_patches)
            attention_output = MultiHeadAttention(self.embed_dim, self.num_heads)(x1)
            x2 = tf.keras.layers.Add()([attention_output, encoded_patches])
            
            # MLP
            x3 = LayerNormalization(epsilon=1e-6)(x2)
            x3 = Dense(self.embed_dim * 4, activation="gelu")(x3)
            x3 = Dropout(0.1)(x3)
            x3 = Dense(self.embed_dim)(x3)
            x3 = Dropout(0.1)(x3)
            encoded_patches = tf.keras.layers.Add()([x3, x2])
        
        # Classification head
        representation = LayerNormalization(epsilon=1e-6)(encoded_patches)
        representation = representation[:, 0]  # Class token
        features = Dense(self.embed_dim, activation="tanh")(representation)
        features = Dropout(0.5)(features)
        logits = Dense(self.num_classes)(features)
        
        model = tf.keras.Model(inputs=inputs, outputs=logits)
        return model
```

#### 2. ğŸ¯ DetecÃ§Ã£o de Objetos

**YOLO v5 Implementation**
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)
        self.bn = nn.BatchNorm2d(out_channels)
        self.activation = nn.SiLU()
    
    def forward(self, x):
        return self.activation(self.bn(self.conv(x)))

class CSPBottleneck(nn.Module):
    def __init__(self, in_channels, out_channels, num_blocks=1, shortcut=True):
        super().__init__()
        hidden_channels = out_channels // 2
        
        self.conv1 = ConvBlock(in_channels, hidden_channels, 1)
        self.conv2 = ConvBlock(in_channels, hidden_channels, 1)
        self.conv3 = ConvBlock(2 * hidden_channels, out_channels, 1)
        
        self.blocks = nn.Sequential(*[ConvBlock(hidden_channels, hidden_channels, 3, padding=1) for _ in range(num_blocks)])
        self.shortcut = shortcut
    
    def forward(self, x):
        x1 = self.conv1(x)
        x2 = self.conv2(x)
        
        x1 = self.blocks(x1)
        
        out = torch.cat((x1, x2), dim=1)
        out = self.conv3(out)
        
        if self.shortcut:
            return x + out
        return out

class YOLOv5(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        # Backbone, Neck, Head implementation
        pass
```

#### 3. ğŸ–¼ï¸ SegmentaÃ§Ã£o SemÃ¢ntica

**U-Net para SegmentaÃ§Ã£o MÃ©dica**
```python
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, concatenate

class UNet:
    def __init__(self, input_shape=(256, 256, 1), num_classes=1):
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.model = self._build_model()
    
    def _conv_block(self, inputs, filters):
        "'''Bloco convolucional duplo"'''
        conv = Conv2D(filters, 3, activation='relu', padding='same')(inputs)
        conv = Conv2D(filters, 3, activation='relu', padding='same')(conv)
        return conv
    
    def _encoder_block(self, inputs, filters):
        "'''Bloco do encoder"'''
        conv = self._conv_block(inputs, filters)
        pool = MaxPooling2D(pool_size=(2, 2))(conv)
        return conv, pool
    
    def _decoder_block(self, inputs, skip_connection, filters):
        "'''Bloco do decoder"'''
        up = UpSampling2D(size=(2, 2))(inputs)
        up = Conv2D(filters, 2, activation='relu', padding='same')(up)
        
        # Skip connection
        merge = concatenate([skip_connection, up], axis=3)
        conv = self._conv_block(merge, filters)
        return conv
    
    def _build_model(self):
        inputs = tf.keras.Input(self.input_shape)
        
        # Encoder (Contracting path)
        conv1, pool1 = self._encoder_block(inputs, 64)
        conv2, pool2 = self._encoder_block(pool1, 128)
        conv3, pool3 = self._encoder_block(pool2, 256)
        conv4, pool4 = self._encoder_block(pool3, 512)
        
        # Bottleneck
        conv5 = self._conv_block(pool4, 1024)
        
        # Decoder (Expanding path)
        up6 = self._decoder_block(conv5, conv4, 512)
        up7 = self._decoder_block(up6, conv3, 256)
        up8 = self._decoder_block(up7, conv2, 128)
        up9 = self._decoder_block(up8, conv1, 64)
        
        # Output layer
        outputs = Conv2D(self.num_classes, 1, activation='softmax')(up9)
        
        model = tf.keras.Model(inputs=inputs, outputs=outputs)
        return model
    
    def compile_model(self):
        self.model.compile(
            optimizer='adam',
            loss='categorical_crossentropy',
            metrics=['accuracy', self._dice_coefficient, self._iou_score]
        )
    
    def _dice_coefficient(self, y_true, y_pred, smooth=1):
        "'''Coeficiente Dice para segmentaÃ§Ã£o"'''
        y_true_f = tf.keras.backend.flatten(y_true)
        y_pred_f = tf.keras.backend.flatten(y_pred)
        intersection = tf.keras.backend.sum(y_true_f * y_pred_f)
        return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)
    
    def _iou_score(self, y_true, y_pred, smooth=1):
        "'''Intersection over Union"'''
        y_true_f = tf.keras.backend.flatten(y_true)
        y_pred_f = tf.keras.backend.flatten(y_pred)
        intersection = tf.keras.backend.sum(y_true_f * y_pred_f)
        union = tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) - intersection
        return (intersection + smooth) / (union + smooth)
```

#### 4. ğŸ‘ï¸ Reconhecimento Facial

**FaceNet Implementation**
```python
import tensorflow as tf
from tensorflow.keras.layers import Dense, Lambda, Input
from tensorflow.keras.models import Model
import numpy as np

class FaceNet:
    def __init__(self, input_shape=(160, 160, 3), embedding_size=128):
        self.input_shape = input_shape
        self.embedding_size = embedding_size
        self.model = self._build_model()
    
    def _build_model(self):
        # Base model (pode ser ResNet, Inception, etc.)
        base_model = tf.keras.applications.InceptionResNetV2(
            input_shape=self.input_shape,
            include_top=False,
            weights='imagenet'
        )
        
        # Adicionar camadas de embedding
        x = base_model.output
        x = tf.keras.layers.GlobalAveragePooling2D()(x)
        x = Dense(self.embedding_size)(x)
        
        # L2 normalization
        embeddings = Lambda(lambda x: tf.nn.l2_normalize(x, axis=1))(x)
        
        model = Model(inputs=base_model.input, outputs=embeddings)
        return model
    
    def triplet_loss(self, alpha=0.2):
        "'''Triplet loss para treinamento"'''
        def loss(y_true, y_pred):
            anchor, positive, negative = y_pred[:, :self.embedding_size], \
                                       y_pred[:, self.embedding_size:2*self.embedding_size], \
                                       y_pred[:, 2*self.embedding_size:]
            
            # DistÃ¢ncias
            pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)
            neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)
            
            # Triplet loss
            basic_loss = pos_dist - neg_dist + alpha
            loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0))
            
            return loss
        return loss
    
    def get_embedding(self, face_image):
        "'''Obter embedding de uma face"'''
        if len(face_image.shape) == 3:
            face_image = np.expand_dims(face_image, axis=0)
        
        embedding = self.model.predict(face_image)
        return embedding[0]
    
    def compare_faces(self, face1, face2, threshold=0.6):
        "'''Comparar duas faces"'''
        emb1 = self.get_embedding(face1)
        emb2 = self.get_embedding(face2)
        
        distance = np.linalg.norm(emb1 - emb2)
        is_same_person = distance < threshold
        
        return {
            'distance': distance,
            'is_same_person': is_same_person,
            'confidence': 1 - (distance / 2)  # Normalizar para 0-1
        }
    
    def face_recognition_pipeline(self, image, known_faces_db):
        "'''Pipeline completo de reconhecimento"'''
        # 1. Detectar faces na imagem
        faces = self._detect_faces(image)
        
        results = []
        for face in faces:
            # 2. Obter embedding da face
            embedding = self.get_embedding(face['image'])
            
            # 3. Comparar com banco de faces conhecidas
            best_match = None
            min_distance = float('inf')
            
            for person_id, known_embedding in known_faces_db.items():
                distance = np.linalg.norm(embedding - known_embedding)
                if distance < min_distance:
                    min_distance = distance
                    best_match = person_id
            
            # 4. Determinar se Ã© uma pessoa conhecida
            if min_distance < 0.6:  # threshold
                results.append({
                    'bbox': face['bbox'],
                    'person_id': best_match,
                    'confidence': 1 - (min_distance / 2),
                    'distance': min_distance
                })
            else:
                results.append({
                    'bbox': face['bbox'],
                    'person_id': 'unknown',
                    'confidence': 0,
                    'distance': min_distance
                })
        
        return results
```

### ğŸ¯ AplicaÃ§Ãµes PrÃ¡ticas

#### 1. ğŸ¥ DiagnÃ³stico MÃ©dico por Imagem

**ClassificaÃ§Ã£o de Raios-X**
```python
class MedicalImageClassifier:
    def __init__(self):
        self.model = self._load_pretrained_model()
        self.classes = ['Normal', 'Pneumonia', 'COVID-19', 'Tuberculosis']
    
    def diagnose_xray(self, xray_image):
        # PrÃ©-processamento
        processed_image = self._preprocess_medical_image(xray_image)
        
        # PrediÃ§Ã£o
        predictions = self.model.predict(processed_image)
        
        # InterpretaÃ§Ã£o
        diagnosis = {
            'primary_diagnosis': self.classes[np.argmax(predictions)],
            'confidence': float(np.max(predictions)),
            'all_probabilities': {
                class_name: float(prob) 
                for class_name, prob in zip(self.classes, predictions[0])
            }
        }
        
        # Mapa de atenÃ§Ã£o (Grad-CAM)
        attention_map = self._generate_gradcam(processed_image)
        
        return {
            'diagnosis': diagnosis,
            'attention_map': attention_map,
            'recommendations': self._get_recommendations(diagnosis)
        }
    
    def _generate_gradcam(self, image):
        "'''Gerar mapa de atenÃ§Ã£o Grad-CAM"'''
        # ImplementaÃ§Ã£o Grad-CAM para interpretabilidade
        pass
```

#### 2. ğŸš— VisÃ£o Computacional Automotiva

**DetecÃ§Ã£o para VeÃ­culos AutÃ´nomos**
```python
class AutomotiveVision:
    def __init__(self):
        self.object_detector = self._load_object_detector()
        self.lane_detector = self._load_lane_detector()
        self.traffic_sign_classifier = self._load_traffic_sign_classifier()
    
    def process_driving_scene(self, image):
        results = {}
        
        # DetecÃ§Ã£o de objetos
        objects = self.object_detector.detect(image)
        results['objects'] = self._filter_automotive_objects(objects)
        
        # DetecÃ§Ã£o de faixas
        lanes = self.lane_detector.detect_lanes(image)
        results['lanes'] = lanes
        
        # ClassificaÃ§Ã£o de sinais de trÃ¢nsito
        traffic_signs = self.traffic_sign_classifier.detect_and_classify(image)
        results['traffic_signs'] = traffic_signs
        
        # AnÃ¡lise de risco
        risk_assessment = self._assess_driving_risk(results)
        results['risk_assessment'] = risk_assessment
        
        return results
    
    def _assess_driving_risk(self, detection_results):
        "'''Avaliar risco baseado nas detecÃ§Ãµes"'''
        risk_factors = []
        
        # Verificar proximidade de pedestres
        for obj in detection_results['objects']:
            if obj['class'] == 'person' and obj['distance'] < 10:
                risk_factors.append('pedestrian_close')
        
        # Verificar veÃ­culos prÃ³ximos
        for obj in detection_results['objects']:
            if obj['class'] in ['car', 'truck'] and obj['distance'] < 5:
                risk_factors.append('vehicle_close')
        
        # Verificar sinais de trÃ¢nsito
        for sign in detection_results['traffic_signs']:
            if sign['class'] == 'stop_sign':
                risk_factors.append('stop_sign_detected')
        
        return {
            'risk_level': len(risk_factors),
            'risk_factors': risk_factors,
            'recommended_action': self._get_recommended_action(risk_factors)
        }
```

#### 3. ğŸ­ Controle de Qualidade Industrial

**InspeÃ§Ã£o Automatizada**
```python
class QualityInspection:
    def __init__(self):
        self.defect_detector = self._load_defect_detection_model()
        self.measurement_model = self._load_measurement_model()
    
    def inspect_product(self, product_image):
        inspection_results = {}
        
        # DetecÃ§Ã£o de defeitos
        defects = self.defect_detector.detect_defects(product_image)
        inspection_results['defects'] = defects
        
        # MediÃ§Ãµes dimensionais
        measurements = self.measurement_model.measure_dimensions(product_image)
        inspection_results['measurements'] = measurements
        
        # ClassificaÃ§Ã£o de qualidade
        quality_score = self._calculate_quality_score(defects, measurements)
        inspection_results['quality_score'] = quality_score
        
        # DecisÃ£o de aprovaÃ§Ã£o/rejeiÃ§Ã£o
        decision = self._make_quality_decision(quality_score, defects)
        inspection_results['decision'] = decision
        
        return inspection_results
    
    def _calculate_quality_score(self, defects, measurements):
        "'''Calcular score de qualidade baseado em defeitos e mediÃ§Ãµes"'''
        base_score = 100
        
        # Penalizar por defeitos
        for defect in defects:
            severity = defect['severity']
            base_score -= severity * 10
        
        # Penalizar por mediÃ§Ãµes fora de especificaÃ§Ã£o
        for measurement in measurements:
            if not measurement['within_tolerance']:
                deviation = measurement['deviation_percentage']
                base_score -= deviation * 5
        
        return max(0, base_score)
```

### ğŸš€ Deployment e OtimizaÃ§Ã£o

#### TensorFlow Lite para Mobile
```python
class MobileDeployment:
    def __init__(self, model_path):
        self.model_path = model_path
    
    def convert_to_tflite(self, quantization=True):
        "'''Converter modelo para TensorFlow Lite"'''
        converter = tf.lite.TFLiteConverter.from_saved_model(self.model_path)
        
        if quantization:
            # QuantizaÃ§Ã£o para reduzir tamanho
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            
            # QuantizaÃ§Ã£o INT8 (opcional)
            converter.target_spec.supported_types = [tf.int8]
            converter.inference_input_type = tf.uint8
            converter.inference_output_type = tf.uint8
        
        tflite_model = converter.convert()
        
        # Salvar modelo
        tflite_path = self.model_path.replace('.pb', '.tflite')
        with open(tflite_path, 'wb') as f:
            f.write(tflite_model)
        
        return tflite_path
    
    def benchmark_model(self, tflite_path, test_images):
        "'''Benchmark de performance do modelo"'''
        interpreter = tf.lite.Interpreter(model_path=tflite_path)
        interpreter.allocate_tensors()
        
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        
        inference_times = []
        
        for image in test_images:
            start_time = time.time()
            
            # Preparar input
            interpreter.set_tensor(input_details[0]['index'], image)
            
            # Executar inferÃªncia
            interpreter.invoke()
            
            # Obter output
            output = interpreter.get_tensor(output_details[0]['index'])
            
            inference_time = time.time() - start_time
            inference_times.append(inference_time)
        
        return {
            'avg_inference_time': np.mean(inference_times),
            'min_inference_time': np.min(inference_times),
            'max_inference_time': np.max(inference_times),
            'fps': 1.0 / np.mean(inference_times)
        }
```

### ğŸ¯ CompetÃªncias Demonstradas

#### Deep Learning
- âœ… **CNNs**: Redes convolucionais clÃ¡ssicas e modernas
- âœ… **Transfer Learning**: Aproveitamento de modelos prÃ©-treinados
- âœ… **Vision Transformers**: Arquiteturas baseadas em atenÃ§Ã£o
- âœ… **GANs**: Redes adversÃ¡rias generativas

#### Computer Vision
- âœ… **ClassificaÃ§Ã£o**: Reconhecimento de imagens
- âœ… **DetecÃ§Ã£o**: LocalizaÃ§Ã£o e classificaÃ§Ã£o de objetos
- âœ… **SegmentaÃ§Ã£o**: SegmentaÃ§Ã£o semÃ¢ntica e de instÃ¢ncia
- âœ… **Reconhecimento Facial**: IdentificaÃ§Ã£o e verificaÃ§Ã£o

#### MLOps para CV
- âœ… **Model Optimization**: QuantizaÃ§Ã£o, pruning, distillation
- âœ… **Edge Deployment**: TensorFlow Lite, ONNX
- âœ… **Performance Monitoring**: LatÃªncia, throughput, accuracy
- âœ… **A/B Testing**: ComparaÃ§Ã£o de modelos em produÃ§Ã£o

### ğŸ“Š Benchmarks de Performance

#### Modelos de ClassificaÃ§Ã£o
| Modelo | ImageNet Top-1 | ParÃ¢metros | FLOPs | LatÃªncia (ms) |
|--------|----------------|------------|-------|---------------|
| ResNet-50 | 76.1% | 25.6M | 4.1G | 15.2 |
| EfficientNet-B0 | 77.1% | 5.3M | 0.39G | 8.7 |
| Vision Transformer | 81.8% | 86M | 17.6G | 45.3 |

#### Modelos de DetecÃ§Ã£o
| Modelo | COCO mAP | FPS | Tamanho |
|--------|----------|-----|---------|
| YOLOv5s | 37.4 | 140 | 14MB |
| YOLOv5m | 45.4 | 85 | 42MB |
| YOLOv5l | 49.0 | 55 | 92MB |

---

## ğŸ‡ºğŸ‡¸ English

### ğŸ”¬ Overview

Comprehensive **Deep Learning for Computer Vision** platform developed in Python, implementing state-of-the-art architectures:

- ğŸ§  **Convolutional Neural Networks**: CNN, ResNet, EfficientNet, Vision Transformer
- ğŸ¯ **Object Detection**: YOLO, R-CNN, SSD, RetinaNet
- ğŸ–¼ï¸ **Segmentation**: U-Net, Mask R-CNN, DeepLab, Semantic Segmentation
- ğŸ‘ï¸ **Face Recognition**: FaceNet, ArcFace, DeepFace
- ğŸ¨ **Image Generation**: GAN, VAE, Diffusion Models
- ğŸ“± **Mobile Deploy**: TensorFlow Lite, ONNX, Edge Computing

### ğŸ¯ Platform Objectives

- **Implement** modern deep learning architectures
- **Facilitate** CV application development
- **Optimize** models for production and edge devices
- **Demonstrate** advanced computer vision techniques
- **Accelerate** prototyping and deployment

### ğŸ”¬ Implemented Models

#### 1. ğŸ–¼ï¸ Image Classification
- ResNet with Transfer Learning
- EfficientNet for efficiency
- Vision Transformer (ViT)
- Custom CNN architectures

#### 2. ğŸ¯ Object Detection
- YOLO v5 implementation
- Faster R-CNN
- Single Shot Detector (SSD)
- RetinaNet with focal loss

#### 3. ğŸ–¼ï¸ Semantic Segmentation
- U-Net for medical imaging
- Mask R-CNN for instance segmentation
- DeepLab v3+ for semantic segmentation
- Fully Convolutional Networks

#### 4. ğŸ‘ï¸ Face Recognition
- FaceNet implementation
- ArcFace for face verification
- DeepFace wrapper
- Face detection and alignment

### ğŸ¯ Skills Demonstrated

#### Deep Learning
- âœ… **CNNs**: Classic and modern convolutional networks
- âœ… **Transfer Learning**: Leveraging pre-trained models
- âœ… **Vision Transformers**: Attention-based architectures
- âœ… **GANs**: Generative adversarial networks

#### Computer Vision
- âœ… **Classification**: Image recognition
- âœ… **Detection**: Object localization and classification
- âœ… **Segmentation**: Semantic and instance segmentation
- âœ… **Face Recognition**: Identification and verification

#### MLOps for CV
- âœ… **Model Optimization**: Quantization, pruning, distillation
- âœ… **Edge Deployment**: TensorFlow Lite, ONNX
- âœ… **Performance Monitoring**: Latency, throughput, accuracy
- âœ… **A/B Testing**: Comparing models in production





---

### âœ’ï¸ Autoria

<div align="center">

**Desenvolvido por Gabriel Demetrios Lafis**

[![GitHub](https://img.shields.io/badge/GitHub-galafis-blue?style=flat-square&logo=github)](https://github.com/galafis)

</div>

